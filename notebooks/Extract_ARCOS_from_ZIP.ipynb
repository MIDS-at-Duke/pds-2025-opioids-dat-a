{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe3ca58",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab26cc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5411423",
   "metadata": {},
   "source": [
    "## Step 2: Locate ARCOS Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764f8820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCOS Data File:\n",
      "============================================================\n",
      "✓ File found: ../data/raw/arcos_all/arcos_all.tsv\n",
      "  Size: 245,729,430,235 bytes (234345.85 MB)\n",
      "  Size: 228.85 GB\n",
      "\n",
      "✓ Ready to process!\n"
     ]
    }
   ],
   "source": [
    "# Path to extracted TSV file\n",
    "tsv_path = '../data/raw/arcos_all/arcos_all.tsv'\n",
    "\n",
    "print(\"ARCOS Data File:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists(tsv_path):\n",
    "    file_size = os.path.getsize(tsv_path)\n",
    "    print(f\"✓ File found: {tsv_path}\")\n",
    "    print(f\"  Size: {file_size:,} bytes ({file_size / (1024**2):.2f} MB)\")\n",
    "    print(f\"  Size: {file_size / (1024**3):.2f} GB\")\n",
    "    print(\"\\n✓ Ready to process!\")\n",
    "else:\n",
    "    print(f\"✗ File not found: {tsv_path}\")\n",
    "    print(\"\\nPlease extract arcos_all.zip first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575afc25",
   "metadata": {},
   "source": [
    "## Step 3: Preview Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a49c1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from: ../data/raw/arcos_all/arcos_all.tsv\n",
      "============================================================\n",
      "\n",
      "First 5 rows:\n",
      "  REPORTER_DEA_NO REPORTER_BUS_ACT         REPORTER_NAME  \\\n",
      "0       RM0220688      DISTRIBUTOR  MCKESSON CORPORATION   \n",
      "1       RM0220688      DISTRIBUTOR  MCKESSON CORPORATION   \n",
      "2       RM0220688      DISTRIBUTOR  MCKESSON CORPORATION   \n",
      "3       RM0220688      DISTRIBUTOR  MCKESSON CORPORATION   \n",
      "4       RM0220688      DISTRIBUTOR  MCKESSON CORPORATION   \n",
      "\n",
      "   REPORTER_ADDL_CO_INFO      REPORTER_ADDRESS1  REPORTER_ADDRESS2  \\\n",
      "0                    NaN  DBA MCKESSON DRUG CO.  3000 KENSKILL AVE   \n",
      "1                    NaN  DBA MCKESSON DRUG CO.  3000 KENSKILL AVE   \n",
      "2                    NaN  DBA MCKESSON DRUG CO.  3000 KENSKILL AVE   \n",
      "3                    NaN  DBA MCKESSON DRUG CO.  3000 KENSKILL AVE   \n",
      "4                    NaN  DBA MCKESSON DRUG CO.  3000 KENSKILL AVE   \n",
      "\n",
      "         REPORTER_CITY REPORTER_STATE  REPORTER_ZIP REPORTER_COUNTY  ...  \\\n",
      "0  WASHINGTON CT HOUSE             OH         43160         FAYETTE  ...   \n",
      "1  WASHINGTON CT HOUSE             OH         43160         FAYETTE  ...   \n",
      "2  WASHINGTON CT HOUSE             OH         43160         FAYETTE  ...   \n",
      "3  WASHINGTON CT HOUSE             OH         43160         FAYETTE  ...   \n",
      "4  WASHINGTON CT HOUSE             OH         43160         FAYETTE  ...   \n",
      "\n",
      "     DRUG_NAME Measure MME_Conversion_Factor Dosage_Strength TRANSACTION_DATE  \\\n",
      "0      CODEINE     TAB                  0.15            30.0       2011-01-14   \n",
      "1    OXYCODONE     TAB                  1.50            10.0       2011-02-08   \n",
      "2  HYDROCODONE     TAB                  1.00             7.5       2011-03-07   \n",
      "3    METHADONE     TAB                  4.00            10.0       2011-03-01   \n",
      "4    OXYCODONE     TAB                  1.50            10.0       2011-03-10   \n",
      "\n",
      "      Combined_Labeler_Name       Reporter_family CALC_BASE_WT_IN_GM  \\\n",
      "0                      Teva  McKesson Corporation            2.21010   \n",
      "1  Par Pharmaceutical, Inc.  McKesson Corporation            0.89650   \n",
      "2                SpecGx LLC  McKesson Corporation            0.45405   \n",
      "3                SpecGx LLC  McKesson Corporation            3.57760   \n",
      "4  Par Pharmaceutical, Inc.  McKesson Corporation            3.58600   \n",
      "\n",
      "   DOSAGE_UNIT        MME  \n",
      "0        100.0    331.515  \n",
      "1        100.0   1344.750  \n",
      "2        100.0    454.050  \n",
      "3        400.0  14310.400  \n",
      "4        400.0   5379.000  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "\n",
      "\n",
      "Columns (33 total):\n",
      "  - REPORTER_DEA_NO\n",
      "  - REPORTER_BUS_ACT\n",
      "  - REPORTER_NAME\n",
      "  - REPORTER_ADDL_CO_INFO\n",
      "  - REPORTER_ADDRESS1\n",
      "  - REPORTER_ADDRESS2\n",
      "  - REPORTER_CITY\n",
      "  - REPORTER_STATE\n",
      "  - REPORTER_ZIP\n",
      "  - REPORTER_COUNTY\n",
      "  - BUYER_DEA_NO\n",
      "  - BUYER_BUS_ACT\n",
      "  - BUYER_NAME\n",
      "  - BUYER_ADDL_CO_INFO\n",
      "  - BUYER_ADDRESS1\n",
      "  - BUYER_ADDRESS2\n",
      "  - BUYER_CITY\n",
      "  - BUYER_STATE\n",
      "  - BUYER_ZIP\n",
      "  - BUYER_COUNTY\n",
      "  - TRANSACTION_CODE\n",
      "  - DRUG_CODE\n",
      "  - NDC_NO\n",
      "  - DRUG_NAME\n",
      "  - Measure\n",
      "  - MME_Conversion_Factor\n",
      "  - Dosage_Strength\n",
      "  - TRANSACTION_DATE\n",
      "  - Combined_Labeler_Name\n",
      "  - Reporter_family\n",
      "  - CALC_BASE_WT_IN_GM\n",
      "  - DOSAGE_UNIT\n",
      "  - MME\n"
     ]
    }
   ],
   "source": [
    "# Preview first few rows\n",
    "print(f\"Reading from: {tsv_path}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_preview = pd.read_csv(tsv_path, sep='\\t', nrows=5)\n",
    "        \n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_preview)\n",
    "\n",
    "print(f\"\\n\\nColumns ({len(df_preview.columns)} total):\")\n",
    "for col in df_preview.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ba401",
   "metadata": {},
   "source": [
    "## Step 4: Define Filters and Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a06961",
   "metadata": {},
   "source": [
    "## Step 4a: Understand the Bottleneck\n",
    "\n",
    "The extraction is slow because:\n",
    "1. Reading from compressed ZIP (decompression overhead)\n",
    "2. Parsing tab-separated text format (slow compared to binary formats)\n",
    "3. Date parsing on millions of rows\n",
    "4. Even with `usecols`, pandas still scans all columns\n",
    "\n",
    "**Alternative approaches:**\n",
    "- Option 1: Use this notebook (slow but works)\n",
    "- Option 2: Extract the TSV from ZIP first, then process (2-step but faster total)\n",
    "- Option 3: Use polars instead of pandas (3-5x faster parsing)\n",
    "- Option 4: If you only need 2006-2015 and 14 states, filter during read (much smaller output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884fec14",
   "metadata": {},
   "source": [
    "## Step 4b: FASTER Alternative - Filter While Reading\n",
    "\n",
    "If you only need specific states/years, filter during extraction to massively reduce output size and speed up processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27e2b149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering configuration:\n",
      "  States: 14 states - FL, WA, GA, AL, SC, NC, TN, MS, OR, CO, MN, NV, CA, VA\n",
      "  Years: 2006-2015\n",
      "\n",
      "This will extract only the filtered data (much smaller output)\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Define filters to reduce output size dramatically\n",
    "# Set to None to disable filtering and extract all data\n",
    "\n",
    "# States to keep (14 states for DiD analysis)\n",
    "states_filter = ['FL', 'WA', 'GA', 'AL', 'SC', 'NC', 'TN', 'MS', 'OR', 'CO', 'MN', 'NV', 'CA', 'VA']\n",
    "\n",
    "# Year range\n",
    "year_min, year_max = 2006, 2015\n",
    "\n",
    "if states_filter is not None:\n",
    "    print(\"Filtering configuration:\")\n",
    "    print(f\"  States: {len(states_filter)} states - {', '.join(states_filter)}\")\n",
    "    print(f\"  Years: {year_min}-{year_max}\")\n",
    "    print(\"\\nThis will extract only the filtered data (much smaller output)\")\n",
    "else:\n",
    "    print(\"Filtering DISABLED - extracting all data\")\n",
    "    print(\"This will save the full dataset with selected columns only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec7e2904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "============================================================\n",
      "\n",
      "Columns to keep (13):\n",
      "\n",
      "ESSENTIAL (for analysis):\n",
      "  ✓ BUYER_STATE\n",
      "  ✓ BUYER_COUNTY\n",
      "  ✓ TRANSACTION_DATE\n",
      "  ✓ MME\n",
      "  ✓ DOSAGE_UNIT\n",
      "  ✓ DRUG_NAME\n",
      "\n",
      "OPTIONAL (for validation/flexibility):\n",
      "  - MME_Conversion_Factor\n",
      "  - Dosage_Strength\n",
      "  - CALC_BASE_WT_IN_GM\n",
      "  - DRUG_CODE\n",
      "  - NDC_NO\n",
      "  - TRANSACTION_CODE\n",
      "  - Measure\n",
      "\n",
      "No filtering applied - keeping all states and years for maximum flexibility\n"
     ]
    }
   ],
   "source": [
    "# Columns to keep - focused on essential data for analysis\n",
    "columns_to_keep = [\n",
    "    # ESSENTIAL - Geographic identifiers\n",
    "    'BUYER_STATE',              # State identifier\n",
    "    'BUYER_COUNTY',             # County name\n",
    "    \n",
    "    # ESSENTIAL - Temporal data\n",
    "    'TRANSACTION_DATE',         # Transaction date (will extract year)\n",
    "    \n",
    "    # ESSENTIAL - Outcome measures\n",
    "    'MME',                      # Morphine Milligram Equivalents (primary outcome)\n",
    "    'DOSAGE_UNIT',              # Number of pills/units (secondary outcome)\n",
    "    \n",
    "    # ESSENTIAL - Drug information\n",
    "    'DRUG_NAME',                # Opioid type (OXYCODONE, HYDROCODONE, etc.)\n",
    "    \n",
    "    # OPTIONAL - For validation and flexibility\n",
    "    'MME_Conversion_Factor',    # For MME validation/recalculation\n",
    "    'Dosage_Strength',          # mg per unit (for MME verification)\n",
    "    'CALC_BASE_WT_IN_GM',       # Total active ingredient weight (for QA)\n",
    "    'DRUG_CODE',                # Drug classification code\n",
    "    'NDC_NO',                   # National Drug Code\n",
    "    'TRANSACTION_CODE',         # Transaction type (to filter sales vs adjustments)\n",
    "    'Measure'                   # Unit type (TAB, CAP, etc.)\n",
    "]\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nColumns to keep ({len(columns_to_keep)}):\")\n",
    "print(\"\\nESSENTIAL (for analysis):\")\n",
    "for col in ['BUYER_STATE', 'BUYER_COUNTY', 'TRANSACTION_DATE', 'MME', 'DOSAGE_UNIT', 'DRUG_NAME']:\n",
    "    print(f\"  ✓ {col}\")\n",
    "print(\"\\nOPTIONAL (for validation/flexibility):\")\n",
    "for col in ['MME_Conversion_Factor', 'Dosage_Strength', 'CALC_BASE_WT_IN_GM', 'DRUG_CODE', 'NDC_NO', 'TRANSACTION_CODE', 'Measure']:\n",
    "    print(f\"  - {col}\")\n",
    "print(f\"\\nNo filtering applied - keeping all states and years for maximum flexibility\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e0b6fd",
   "metadata": {},
   "source": [
    "## Step 4c: TEST - Create Small Sample to Verify Columns\n",
    "\n",
    "Before processing the full 228GB file, let's create a small sample to verify all columns (especially Dosage_Strength) are being saved correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebbf6849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING: Creating small sample parquet file\n",
      "============================================================\n",
      "\n",
      "1. Reading sample (100,000 rows) from TSV...\n",
      "   ✓ Read 100,000 rows\n",
      "   Columns: 14\n",
      "\n",
      "2. Saving test file: ../data/raw/TEST_sample.parquet\n",
      "   ✓ Saved! Size: 0.86 MB\n",
      "\n",
      "3. Verifying by reading back...\n",
      "   ✓ Read back 100,000 rows × 14 columns\n",
      "\n",
      "   Columns in saved file:\n",
      "       1. BUYER_STATE                    (nulls:      0)\n",
      "       2. BUYER_COUNTY                   (nulls:      1)\n",
      "       3. TRANSACTION_CODE               (nulls:      0)\n",
      "       4. DRUG_CODE                      (nulls:      0)\n",
      "       5. NDC_NO                         (nulls:      0)\n",
      "       6. DRUG_NAME                      (nulls:      0)\n",
      "       7. Measure                        (nulls:      4)\n",
      "       8. MME_Conversion_Factor          (nulls:      0)\n",
      "       9. Dosage_Strength                (nulls:      0)\n",
      "      10. TRANSACTION_DATE               (nulls:      0)\n",
      "      11. CALC_BASE_WT_IN_GM             (nulls:      0)\n",
      "      12. DOSAGE_UNIT                    (nulls: 14,139)\n",
      "      13. MME                            (nulls:      0)\n",
      "      14. year                           (nulls:      0)\n",
      "\n",
      "4. Critical Column Check:\n",
      "   ✓ Dosage_Strength present: True\n",
      "   ✓ CALC_BASE_WT_IN_GM present: True\n",
      "   ✓ MME_Conversion_Factor present: True\n",
      "\n",
      "5. Sample data with key columns:\n",
      "  BUYER_STATE    DRUG_NAME  Dosage_Strength  CALC_BASE_WT_IN_GM  DOSAGE_UNIT  \\\n",
      "0          IN      CODEINE             30.0             2.21010        100.0   \n",
      "1          IN    OXYCODONE             10.0             0.89650        100.0   \n",
      "2          IN  HYDROCODONE              7.5             0.45405        100.0   \n",
      "3          IN    METHADONE             10.0             3.57760        400.0   \n",
      "4          IN    OXYCODONE             10.0             3.58600        400.0   \n",
      "5          IN  HYDROCODONE              5.0             1.51350        500.0   \n",
      "6          IN  HYDROCODONE              5.0             0.30270        100.0   \n",
      "7          IN    OXYCODONE             10.0             1.79300        200.0   \n",
      "8          IN    OXYCODONE             10.0             0.89650        100.0   \n",
      "9          IN     FENTANYL              0.1             0.30600         30.0   \n",
      "\n",
      "            MME  year  \n",
      "0    331.515015  2011  \n",
      "1   1344.750000  2011  \n",
      "2    454.049988  2011  \n",
      "3  14310.400391  2011  \n",
      "4   5379.000000  2011  \n",
      "5   1513.500000  2011  \n",
      "6    302.700012  2011  \n",
      "7   2689.500000  2011  \n",
      "8   1344.750000  2011  \n",
      "9  30600.000000  2011  \n",
      "\n",
      "============================================================\n",
      "✅ SUCCESS! Dosage_Strength is being saved correctly!\n",
      "   You can proceed with the full extraction.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TESTING: Creating small sample parquet file\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the same configuration as the full extraction\n",
    "test_columns = columns_to_keep.copy()\n",
    "test_dtypes = {\n",
    "    'BUYER_STATE': str,\n",
    "    'BUYER_COUNTY': str,\n",
    "    'DRUG_NAME': str,\n",
    "    'DRUG_CODE': str,\n",
    "    'TRANSACTION_CODE': str,\n",
    "    'Measure': str,\n",
    "    'NDC_NO': str,\n",
    "    'MME': 'float32',\n",
    "    'DOSAGE_UNIT': 'float32',\n",
    "    'MME_Conversion_Factor': 'float32',\n",
    "    'Dosage_Strength': 'float32',\n",
    "    'CALC_BASE_WT_IN_GM': 'float32'\n",
    "}\n",
    "\n",
    "print(f\"\\n1. Reading sample (100,000 rows) from TSV...\")\n",
    "df_test = pd.read_csv(\n",
    "    tsv_path,\n",
    "    sep='\\t',\n",
    "    usecols=test_columns,\n",
    "    dtype=test_dtypes,\n",
    "    nrows=100000\n",
    ")\n",
    "\n",
    "# Add year column (same as full extraction)\n",
    "df_test['year'] = pd.to_datetime(df_test['TRANSACTION_DATE'], format='%Y-%m-%d', errors='coerce').dt.year.astype('Int16')\n",
    "\n",
    "print(f\"   ✓ Read {len(df_test):,} rows\")\n",
    "print(f\"   Columns: {len(df_test.columns)}\")\n",
    "\n",
    "# Save test file\n",
    "test_output = '../data/raw/TEST_sample.parquet'\n",
    "print(f\"\\n2. Saving test file: {test_output}\")\n",
    "df_test.to_parquet(test_output, index=False, compression='snappy')\n",
    "\n",
    "file_size = os.path.getsize(test_output)\n",
    "print(f\"   ✓ Saved! Size: {file_size / (1024**2):.2f} MB\")\n",
    "\n",
    "# Verify by reading it back\n",
    "print(f\"\\n3. Verifying by reading back...\")\n",
    "df_verify = pd.read_parquet(test_output)\n",
    "\n",
    "print(f\"   ✓ Read back {len(df_verify):,} rows × {df_verify.shape[1]} columns\")\n",
    "print(f\"\\n   Columns in saved file:\")\n",
    "for i, col in enumerate(df_verify.columns, 1):\n",
    "    null_count = df_verify[col].isna().sum()\n",
    "    print(f\"      {i:2d}. {col:30s} (nulls: {null_count:6,})\")\n",
    "\n",
    "# Check critical columns\n",
    "print(f\"\\n4. Critical Column Check:\")\n",
    "print(f\"   ✓ Dosage_Strength present: {'Dosage_Strength' in df_verify.columns}\")\n",
    "print(f\"   ✓ CALC_BASE_WT_IN_GM present: {'CALC_BASE_WT_IN_GM' in df_verify.columns}\")\n",
    "print(f\"   ✓ MME_Conversion_Factor present: {'MME_Conversion_Factor' in df_verify.columns}\")\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\n5. Sample data with key columns:\")\n",
    "print(df_verify[['BUYER_STATE', 'DRUG_NAME', 'Dosage_Strength', 'CALC_BASE_WT_IN_GM', 'DOSAGE_UNIT', 'MME', 'year']].head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if 'Dosage_Strength' in df_verify.columns:\n",
    "    print(\"✅ SUCCESS! Dosage_Strength is being saved correctly!\")\n",
    "    print(\"   You can proceed with the full extraction.\")\n",
    "else:\n",
    "    print(\"❌ PROBLEM! Dosage_Strength is missing from saved file!\")\n",
    "    print(\"   Something is wrong with the configuration.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4632c",
   "metadata": {},
   "source": [
    "## Step 5: Extract All Data to Parquet (Optimized)\n",
    "\n",
    "**Note:** This extracts ALL data without filtering. You can filter later in analysis notebooks for flexibility.\n",
    "\n",
    "**Optimizations applied:**\n",
    "- Larger chunk size (500k rows) for fewer iterations\n",
    "- Pre-specified data types (category for strings, float32 instead of float64)\n",
    "- Optimized date parsing\n",
    "- More frequent progress updates\n",
    "\n",
    "**Estimated time:** 5-10 minutes (3-4x faster than default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a2d2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ARCOS data (Optimized)...\n",
      "============================================================\n",
      "FILTERING ENABLED: 14 states, years 2006-2015\n",
      "\n",
      "Reading TSV file in chunks (2M rows each)...\n",
      "\n",
      "Chunk 1: 2,000,000 rows → 0 kept (0.0%) | Total read: 2,000,000\n",
      "Chunk 2: 2,000,000 rows → 0 kept (0.0%) | Total read: 4,000,000\n",
      "Chunk 3: 2,000,000 rows → 0 kept (0.0%) | Total read: 6,000,000\n",
      "Chunk 4: 2,000,000 rows → 0 kept (0.0%) | Total read: 8,000,000\n",
      "Chunk 5: 2,000,000 rows → 0 kept (0.0%) | Total read: 10,000,000\n",
      "Chunk 6: 2,000,000 rows → 0 kept (0.0%) | Total read: 12,000,000\n",
      "Chunk 7: 2,000,000 rows → 0 kept (0.0%) | Total read: 14,000,000\n",
      "Chunk 8: 2,000,000 rows → 0 kept (0.0%) | Total read: 16,000,000\n",
      "Chunk 9: 2,000,000 rows → 440,201 kept (22.0%) | Total read: 18,000,000\n",
      "Chunk 10: 2,000,000 rows → 0 kept (0.0%) | Total read: 20,000,000\n",
      "Chunk 11: 2,000,000 rows → 431,292 kept (21.6%) | Total read: 22,000,000\n",
      "Chunk 12: 2,000,000 rows → 0 kept (0.0%) | Total read: 24,000,000\n",
      "Chunk 13: 2,000,000 rows → 321,473 kept (16.1%) | Total read: 26,000,000\n",
      "Chunk 14: 2,000,000 rows → 98,162 kept (4.9%) | Total read: 28,000,000\n",
      "Chunk 15: 2,000,000 rows → 204,399 kept (10.2%) | Total read: 30,000,000\n",
      "Chunk 16: 2,000,000 rows → 291,861 kept (14.6%) | Total read: 32,000,000\n",
      "Chunk 17: 2,000,000 rows → 293,163 kept (14.7%) | Total read: 34,000,000\n",
      "Chunk 18: 2,000,000 rows → 345,170 kept (17.3%) | Total read: 36,000,000\n",
      "Chunk 19: 2,000,000 rows → 303,448 kept (15.2%) | Total read: 38,000,000\n",
      "Chunk 20: 2,000,000 rows → 0 kept (0.0%) | Total read: 40,000,000\n",
      "Chunk 21: 2,000,000 rows → 24,954 kept (1.2%) | Total read: 42,000,000\n",
      "Chunk 22: 2,000,000 rows → 207,088 kept (10.4%) | Total read: 44,000,000\n",
      "Chunk 23: 2,000,000 rows → 0 kept (0.0%) | Total read: 46,000,000\n",
      "Chunk 24: 2,000,000 rows → 0 kept (0.0%) | Total read: 48,000,000\n",
      "Chunk 25: 2,000,000 rows → 507,311 kept (25.4%) | Total read: 50,000,000\n",
      "Chunk 26: 2,000,000 rows → 207,485 kept (10.4%) | Total read: 52,000,000\n",
      "Chunk 27: 2,000,000 rows → 217,650 kept (10.9%) | Total read: 54,000,000\n",
      "Chunk 28: 2,000,000 rows → 205,007 kept (10.3%) | Total read: 56,000,000\n",
      "Chunk 29: 2,000,000 rows → 294,724 kept (14.7%) | Total read: 58,000,000\n",
      "Chunk 30: 2,000,000 rows → 0 kept (0.0%) | Total read: 60,000,000\n",
      "Chunk 31: 2,000,000 rows → 0 kept (0.0%) | Total read: 62,000,000\n",
      "Chunk 32: 2,000,000 rows → 366,335 kept (18.3%) | Total read: 64,000,000\n",
      "Chunk 33: 2,000,000 rows → 708,837 kept (35.4%) | Total read: 66,000,000\n",
      "Chunk 34: 2,000,000 rows → 0 kept (0.0%) | Total read: 68,000,000\n",
      "Chunk 35: 2,000,000 rows → 701,779 kept (35.1%) | Total read: 70,000,000\n",
      "Chunk 36: 2,000,000 rows → 1,172,345 kept (58.6%) | Total read: 72,000,000\n",
      "Chunk 37: 2,000,000 rows → 0 kept (0.0%) | Total read: 74,000,000\n",
      "Chunk 38: 2,000,000 rows → 674,343 kept (33.7%) | Total read: 76,000,000\n",
      "Chunk 39: 2,000,000 rows → 0 kept (0.0%) | Total read: 78,000,000\n",
      "Chunk 40: 2,000,000 rows → 0 kept (0.0%) | Total read: 80,000,000\n",
      "Chunk 41: 2,000,000 rows → 0 kept (0.0%) | Total read: 82,000,000\n",
      "Chunk 42: 2,000,000 rows → 1,028 kept (0.1%) | Total read: 84,000,000\n",
      "Chunk 43: 2,000,000 rows → 417,557 kept (20.9%) | Total read: 86,000,000\n",
      "Chunk 44: 2,000,000 rows → 0 kept (0.0%) | Total read: 88,000,000\n",
      "Chunk 45: 2,000,000 rows → 444,608 kept (22.2%) | Total read: 90,000,000\n",
      "Chunk 46: 2,000,000 rows → 154,885 kept (7.7%) | Total read: 92,000,000\n",
      "Chunk 47: 2,000,000 rows → 293,896 kept (14.7%) | Total read: 94,000,000\n",
      "Chunk 48: 2,000,000 rows → 159,608 kept (8.0%) | Total read: 96,000,000\n",
      "Chunk 49: 2,000,000 rows → 319,856 kept (16.0%) | Total read: 98,000,000\n",
      "Chunk 50: 2,000,000 rows → 593,550 kept (29.7%) | Total read: 100,000,000\n",
      "Chunk 51: 2,000,000 rows → 0 kept (0.0%) | Total read: 102,000,000\n",
      "Chunk 52: 2,000,000 rows → 145,653 kept (7.3%) | Total read: 104,000,000\n",
      "Chunk 53: 2,000,000 rows → 258,846 kept (12.9%) | Total read: 106,000,000\n",
      "Chunk 54: 2,000,000 rows → 945,934 kept (47.3%) | Total read: 108,000,000\n",
      "Chunk 55: 2,000,000 rows → 561,502 kept (28.1%) | Total read: 110,000,000\n",
      "Chunk 56: 2,000,000 rows → 127,021 kept (6.4%) | Total read: 112,000,000\n",
      "Chunk 57: 2,000,000 rows → 241,557 kept (12.1%) | Total read: 114,000,000\n",
      "Chunk 58: 2,000,000 rows → 354,557 kept (17.7%) | Total read: 116,000,000\n",
      "Chunk 59: 2,000,000 rows → 0 kept (0.0%) | Total read: 118,000,000\n",
      "Chunk 60: 2,000,000 rows → 325,267 kept (16.3%) | Total read: 120,000,000\n",
      "Chunk 61: 2,000,000 rows → 0 kept (0.0%) | Total read: 122,000,000\n",
      "Chunk 62: 2,000,000 rows → 488,984 kept (24.4%) | Total read: 124,000,000\n",
      "Chunk 63: 2,000,000 rows → 258,954 kept (12.9%) | Total read: 126,000,000\n",
      "Chunk 64: 2,000,000 rows → 225,130 kept (11.3%) | Total read: 128,000,000\n",
      "Chunk 65: 2,000,000 rows → 138,238 kept (6.9%) | Total read: 130,000,000\n",
      "Chunk 66: 2,000,000 rows → 452,442 kept (22.6%) | Total read: 132,000,000\n",
      "Chunk 67: 2,000,000 rows → 0 kept (0.0%) | Total read: 134,000,000\n",
      "Chunk 68: 2,000,000 rows → 262,755 kept (13.1%) | Total read: 136,000,000\n",
      "Chunk 69: 2,000,000 rows → 265,566 kept (13.3%) | Total read: 138,000,000\n",
      "Chunk 70: 2,000,000 rows → 268,935 kept (13.4%) | Total read: 140,000,000\n",
      "Chunk 71: 2,000,000 rows → 0 kept (0.0%) | Total read: 142,000,000\n",
      "Chunk 72: 2,000,000 rows → 147,210 kept (7.4%) | Total read: 144,000,000\n",
      "Chunk 73: 2,000,000 rows → 0 kept (0.0%) | Total read: 146,000,000\n",
      "Chunk 74: 2,000,000 rows → 0 kept (0.0%) | Total read: 148,000,000\n",
      "Chunk 75: 2,000,000 rows → 0 kept (0.0%) | Total read: 150,000,000\n",
      "Chunk 76: 2,000,000 rows → 210,187 kept (10.5%) | Total read: 152,000,000\n",
      "Chunk 77: 2,000,000 rows → 0 kept (0.0%) | Total read: 154,000,000\n",
      "Chunk 78: 2,000,000 rows → 0 kept (0.0%) | Total read: 156,000,000\n",
      "Chunk 79: 2,000,000 rows → 0 kept (0.0%) | Total read: 158,000,000\n",
      "Chunk 80: 2,000,000 rows → 137,470 kept (6.9%) | Total read: 160,000,000\n",
      "Chunk 81: 2,000,000 rows → 0 kept (0.0%) | Total read: 162,000,000\n",
      "Chunk 82: 2,000,000 rows → 206,331 kept (10.3%) | Total read: 164,000,000\n",
      "Chunk 83: 2,000,000 rows → 0 kept (0.0%) | Total read: 166,000,000\n",
      "Chunk 84: 2,000,000 rows → 0 kept (0.0%) | Total read: 168,000,000\n",
      "Chunk 85: 2,000,000 rows → 204,782 kept (10.2%) | Total read: 170,000,000\n",
      "Chunk 86: 2,000,000 rows → 354,799 kept (17.7%) | Total read: 172,000,000\n",
      "Chunk 87: 2,000,000 rows → 187,173 kept (9.4%) | Total read: 174,000,000\n",
      "Chunk 88: 2,000,000 rows → 249,741 kept (12.5%) | Total read: 176,000,000\n",
      "Chunk 89: 2,000,000 rows → 222,396 kept (11.1%) | Total read: 178,000,000\n",
      "Chunk 90: 2,000,000 rows → 0 kept (0.0%) | Total read: 180,000,000\n",
      "Chunk 91: 2,000,000 rows → 0 kept (0.0%) | Total read: 182,000,000\n",
      "Chunk 92: 2,000,000 rows → 0 kept (0.0%) | Total read: 184,000,000\n",
      "Chunk 93: 2,000,000 rows → 0 kept (0.0%) | Total read: 186,000,000\n",
      "Chunk 94: 2,000,000 rows → 0 kept (0.0%) | Total read: 188,000,000\n",
      "Chunk 95: 2,000,000 rows → 171,277 kept (8.6%) | Total read: 190,000,000\n",
      "Chunk 96: 2,000,000 rows → 155,111 kept (7.8%) | Total read: 192,000,000\n",
      "Chunk 97: 2,000,000 rows → 0 kept (0.0%) | Total read: 194,000,000\n",
      "Chunk 98: 2,000,000 rows → 0 kept (0.0%) | Total read: 196,000,000\n",
      "Chunk 99: 2,000,000 rows → 358,623 kept (17.9%) | Total read: 198,000,000\n",
      "Chunk 100: 2,000,000 rows → 185,164 kept (9.3%) | Total read: 200,000,000\n",
      "Chunk 101: 2,000,000 rows → 44,643 kept (2.2%) | Total read: 202,000,000\n",
      "Chunk 102: 2,000,000 rows → 51,194 kept (2.6%) | Total read: 204,000,000\n",
      "Chunk 103: 2,000,000 rows → 267,307 kept (13.4%) | Total read: 206,000,000\n",
      "Chunk 104: 2,000,000 rows → 17,209 kept (0.9%) | Total read: 208,000,000\n",
      "Chunk 105: 2,000,000 rows → 0 kept (0.0%) | Total read: 210,000,000\n",
      "Chunk 106: 2,000,000 rows → 282,359 kept (14.1%) | Total read: 212,000,000\n",
      "Chunk 107: 2,000,000 rows → 379,323 kept (19.0%) | Total read: 214,000,000\n",
      "Chunk 108: 2,000,000 rows → 0 kept (0.0%) | Total read: 216,000,000\n",
      "Chunk 109: 2,000,000 rows → 0 kept (0.0%) | Total read: 218,000,000\n",
      "Chunk 110: 2,000,000 rows → 0 kept (0.0%) | Total read: 220,000,000\n",
      "Chunk 111: 2,000,000 rows → 0 kept (0.0%) | Total read: 222,000,000\n",
      "Chunk 112: 2,000,000 rows → 0 kept (0.0%) | Total read: 224,000,000\n",
      "Chunk 113: 2,000,000 rows → 0 kept (0.0%) | Total read: 226,000,000\n",
      "Chunk 114: 2,000,000 rows → 1,428,500 kept (71.4%) | Total read: 228,000,000\n",
      "Chunk 115: 2,000,000 rows → 680,016 kept (34.0%) | Total read: 230,000,000\n",
      "Chunk 116: 2,000,000 rows → 695,068 kept (34.8%) | Total read: 232,000,000\n",
      "Chunk 117: 2,000,000 rows → 0 kept (0.0%) | Total read: 234,000,000\n",
      "Chunk 118: 2,000,000 rows → 0 kept (0.0%) | Total read: 236,000,000\n",
      "Chunk 119: 2,000,000 rows → 0 kept (0.0%) | Total read: 238,000,000\n",
      "Chunk 120: 2,000,000 rows → 0 kept (0.0%) | Total read: 240,000,000\n",
      "Chunk 121: 2,000,000 rows → 297,521 kept (14.9%) | Total read: 242,000,000\n",
      "Chunk 122: 2,000,000 rows → 520,660 kept (26.0%) | Total read: 244,000,000\n",
      "Chunk 123: 2,000,000 rows → 330,078 kept (16.5%) | Total read: 246,000,000\n",
      "Chunk 124: 2,000,000 rows → 0 kept (0.0%) | Total read: 248,000,000\n",
      "Chunk 125: 2,000,000 rows → 120,574 kept (6.0%) | Total read: 250,000,000\n",
      "Chunk 126: 2,000,000 rows → 164 kept (0.0%) | Total read: 252,000,000\n",
      "Chunk 127: 2,000,000 rows → 0 kept (0.0%) | Total read: 254,000,000\n",
      "Chunk 128: 2,000,000 rows → 0 kept (0.0%) | Total read: 256,000,000\n",
      "Chunk 129: 2,000,000 rows → 273,361 kept (13.7%) | Total read: 258,000,000\n",
      "Chunk 130: 2,000,000 rows → 571,135 kept (28.6%) | Total read: 260,000,000\n",
      "Chunk 131: 2,000,000 rows → 0 kept (0.0%) | Total read: 262,000,000\n",
      "Chunk 132: 2,000,000 rows → 550 kept (0.0%) | Total read: 264,000,000\n",
      "Chunk 133: 2,000,000 rows → 190 kept (0.0%) | Total read: 266,000,000\n",
      "Chunk 134: 2,000,000 rows → 0 kept (0.0%) | Total read: 268,000,000\n",
      "Chunk 135: 2,000,000 rows → 811,365 kept (40.6%) | Total read: 270,000,000\n",
      "Chunk 136: 2,000,000 rows → 0 kept (0.0%) | Total read: 272,000,000\n",
      "Chunk 137: 2,000,000 rows → 0 kept (0.0%) | Total read: 274,000,000\n",
      "Chunk 138: 2,000,000 rows → 952,793 kept (47.6%) | Total read: 276,000,000\n",
      "Chunk 139: 2,000,000 rows → 0 kept (0.0%) | Total read: 278,000,000\n",
      "Chunk 140: 2,000,000 rows → 0 kept (0.0%) | Total read: 280,000,000\n",
      "Chunk 141: 2,000,000 rows → 652 kept (0.0%) | Total read: 282,000,000\n",
      "Chunk 142: 2,000,000 rows → 0 kept (0.0%) | Total read: 284,000,000\n",
      "Chunk 143: 2,000,000 rows → 743,420 kept (37.2%) | Total read: 286,000,000\n",
      "Chunk 144: 2,000,000 rows → 0 kept (0.0%) | Total read: 288,000,000\n",
      "Chunk 145: 2,000,000 rows → 0 kept (0.0%) | Total read: 290,000,000\n",
      "Chunk 146: 2,000,000 rows → 0 kept (0.0%) | Total read: 292,000,000\n",
      "Chunk 147: 2,000,000 rows → 1,000,180 kept (50.0%) | Total read: 294,000,000\n",
      "Chunk 148: 2,000,000 rows → 0 kept (0.0%) | Total read: 296,000,000\n",
      "Chunk 149: 2,000,000 rows → 0 kept (0.0%) | Total read: 298,000,000\n",
      "Chunk 150: 2,000,000 rows → 154 kept (0.0%) | Total read: 300,000,000\n",
      "Chunk 151: 2,000,000 rows → 1,596,328 kept (79.8%) | Total read: 302,000,000\n",
      "Chunk 152: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 304,000,000\n",
      "Chunk 153: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 306,000,000\n",
      "Chunk 154: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 308,000,000\n",
      "Chunk 155: 2,000,000 rows → 0 kept (0.0%) | Total read: 310,000,000\n",
      "Chunk 156: 2,000,000 rows → 996,244 kept (49.8%) | Total read: 312,000,000\n",
      "Chunk 157: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 314,000,000\n",
      "Chunk 158: 2,000,000 rows → 1,993,845 kept (99.7%) | Total read: 316,000,000\n",
      "Chunk 159: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 318,000,000\n",
      "Chunk 160: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 320,000,000\n",
      "Chunk 161: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 322,000,000\n",
      "Chunk 162: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 324,000,000\n",
      "Chunk 163: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 326,000,000\n",
      "Chunk 164: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 328,000,000\n",
      "Chunk 165: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 330,000,000\n",
      "Chunk 166: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 332,000,000\n",
      "Chunk 167: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 334,000,000\n",
      "Chunk 168: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 336,000,000\n",
      "Chunk 169: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 338,000,000\n",
      "Chunk 170: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 340,000,000\n",
      "Chunk 171: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 342,000,000\n",
      "Chunk 172: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 344,000,000\n",
      "Chunk 173: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 346,000,000\n",
      "Chunk 174: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 348,000,000\n",
      "Chunk 175: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 350,000,000\n",
      "Chunk 176: 2,000,000 rows → 0 kept (0.0%) | Total read: 352,000,000\n",
      "Chunk 177: 2,000,000 rows → 0 kept (0.0%) | Total read: 354,000,000\n",
      "Chunk 178: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 356,000,000\n",
      "Chunk 179: 2,000,000 rows → 0 kept (0.0%) | Total read: 358,000,000\n",
      "Chunk 180: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 360,000,000\n",
      "Chunk 181: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 362,000,000\n",
      "Chunk 182: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 364,000,000\n",
      "Chunk 183: 2,000,000 rows → 0 kept (0.0%) | Total read: 366,000,000\n",
      "Chunk 184: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 368,000,000\n",
      "Chunk 185: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 370,000,000\n",
      "Chunk 186: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 372,000,000\n",
      "Chunk 187: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 374,000,000\n",
      "Chunk 188: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 376,000,000\n",
      "Chunk 189: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 378,000,000\n",
      "Chunk 190: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 380,000,000\n",
      "Chunk 191: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 382,000,000\n",
      "Chunk 192: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 384,000,000\n",
      "Chunk 193: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 386,000,000\n",
      "Chunk 194: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 388,000,000\n",
      "Chunk 195: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 390,000,000\n",
      "Chunk 196: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 392,000,000\n",
      "Chunk 197: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 394,000,000\n",
      "Chunk 198: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 396,000,000\n",
      "Chunk 199: 2,000,000 rows → 0 kept (0.0%) | Total read: 398,000,000\n",
      "Chunk 200: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 400,000,000\n",
      "Chunk 201: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 402,000,000\n",
      "Chunk 202: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 404,000,000\n",
      "Chunk 203: 2,000,000 rows → 0 kept (0.0%) | Total read: 406,000,000\n",
      "Chunk 204: 2,000,000 rows → 0 kept (0.0%) | Total read: 408,000,000\n",
      "Chunk 205: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 410,000,000\n",
      "Chunk 206: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 412,000,000\n",
      "Chunk 207: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 414,000,000\n",
      "Chunk 208: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 416,000,000\n",
      "Chunk 209: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 418,000,000\n",
      "Chunk 210: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 420,000,000\n",
      "Chunk 211: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 422,000,000\n",
      "Chunk 212: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 424,000,000\n",
      "Chunk 213: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 426,000,000\n",
      "Chunk 214: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 428,000,000\n",
      "Chunk 215: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 430,000,000\n",
      "Chunk 216: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 432,000,000\n",
      "Chunk 217: 2,000,000 rows → 0 kept (0.0%) | Total read: 434,000,000\n",
      "Chunk 218: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 436,000,000\n",
      "Chunk 219: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 438,000,000\n",
      "Chunk 220: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 440,000,000\n",
      "Chunk 221: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 442,000,000\n",
      "Chunk 222: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 444,000,000\n",
      "Chunk 223: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 446,000,000\n",
      "Chunk 224: 2,000,000 rows → 0 kept (0.0%) | Total read: 448,000,000\n",
      "Chunk 225: 2,000,000 rows → 0 kept (0.0%) | Total read: 450,000,000\n",
      "Chunk 226: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 452,000,000\n",
      "Chunk 227: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 454,000,000\n",
      "Chunk 228: 2,000,000 rows → 0 kept (0.0%) | Total read: 456,000,000\n",
      "Chunk 229: 2,000,000 rows → 0 kept (0.0%) | Total read: 458,000,000\n",
      "Chunk 230: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 460,000,000\n",
      "Chunk 231: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 462,000,000\n",
      "Chunk 232: 2,000,000 rows → 0 kept (0.0%) | Total read: 464,000,000\n",
      "Chunk 233: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 466,000,000\n",
      "Chunk 234: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 468,000,000\n",
      "Chunk 235: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 470,000,000\n",
      "Chunk 236: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 472,000,000\n",
      "Chunk 237: 2,000,000 rows → 0 kept (0.0%) | Total read: 474,000,000\n",
      "Chunk 238: 2,000,000 rows → 614 kept (0.0%) | Total read: 476,000,000\n",
      "Chunk 239: 2,000,000 rows → 0 kept (0.0%) | Total read: 478,000,000\n",
      "Chunk 240: 2,000,000 rows → 0 kept (0.0%) | Total read: 480,000,000\n",
      "Chunk 241: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 482,000,000\n",
      "Chunk 242: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 484,000,000\n",
      "Chunk 243: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 486,000,000\n",
      "Chunk 244: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 488,000,000\n",
      "Chunk 245: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 490,000,000\n",
      "Chunk 246: 2,000,000 rows → 0 kept (0.0%) | Total read: 492,000,000\n",
      "Chunk 247: 2,000,000 rows → 0 kept (0.0%) | Total read: 494,000,000\n",
      "Chunk 248: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 496,000,000\n",
      "Chunk 249: 2,000,000 rows → 0 kept (0.0%) | Total read: 498,000,000\n",
      "Chunk 250: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 500,000,000\n",
      "Chunk 251: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 502,000,000\n",
      "Chunk 252: 2,000,000 rows → 0 kept (0.0%) | Total read: 504,000,000\n",
      "Chunk 253: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 506,000,000\n",
      "Chunk 254: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 508,000,000\n",
      "Chunk 255: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 510,000,000\n",
      "Chunk 256: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 512,000,000\n",
      "Chunk 257: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 514,000,000\n",
      "Chunk 258: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 516,000,000\n",
      "Chunk 259: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 518,000,000\n",
      "Chunk 260: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 520,000,000\n",
      "Chunk 261: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 522,000,000\n",
      "Chunk 262: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 524,000,000\n",
      "Chunk 263: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 526,000,000\n",
      "Chunk 264: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 528,000,000\n",
      "Chunk 265: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 530,000,000\n",
      "Chunk 266: 2,000,000 rows → 0 kept (0.0%) | Total read: 532,000,000\n",
      "Chunk 267: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 534,000,000\n",
      "Chunk 268: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 536,000,000\n",
      "Chunk 269: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 538,000,000\n",
      "Chunk 270: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 540,000,000\n",
      "Chunk 271: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 542,000,000\n",
      "Chunk 272: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 544,000,000\n",
      "Chunk 273: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 546,000,000\n",
      "Chunk 274: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 548,000,000\n",
      "Chunk 275: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 550,000,000\n",
      "Chunk 276: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 552,000,000\n",
      "Chunk 277: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 554,000,000\n",
      "Chunk 278: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 556,000,000\n",
      "Chunk 279: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 558,000,000\n",
      "Chunk 280: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 560,000,000\n",
      "Chunk 281: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 562,000,000\n",
      "Chunk 282: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 564,000,000\n",
      "Chunk 283: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 566,000,000\n",
      "Chunk 284: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 568,000,000\n",
      "Chunk 285: 2,000,000 rows → 0 kept (0.0%) | Total read: 570,000,000\n",
      "Chunk 286: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 572,000,000\n",
      "Chunk 287: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 574,000,000\n",
      "Chunk 288: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 576,000,000\n",
      "Chunk 289: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 578,000,000\n",
      "Chunk 290: 2,000,000 rows → 0 kept (0.0%) | Total read: 580,000,000\n",
      "Chunk 291: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 582,000,000\n",
      "Chunk 292: 2,000,000 rows → 0 kept (0.0%) | Total read: 584,000,000\n",
      "Chunk 293: 2,000,000 rows → 0 kept (0.0%) | Total read: 586,000,000\n",
      "Chunk 294: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 588,000,000\n",
      "Chunk 295: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 590,000,000\n",
      "Chunk 296: 2,000,000 rows → 0 kept (0.0%) | Total read: 592,000,000\n",
      "Chunk 297: 2,000,000 rows → 0 kept (0.0%) | Total read: 594,000,000\n",
      "Chunk 298: 2,000,000 rows → 0 kept (0.0%) | Total read: 596,000,000\n",
      "Chunk 299: 2,000,000 rows → 0 kept (0.0%) | Total read: 598,000,000\n",
      "Chunk 300: 2,000,000 rows → 0 kept (0.0%) | Total read: 600,000,000\n",
      "Chunk 301: 2,000,000 rows → 0 kept (0.0%) | Total read: 602,000,000\n",
      "Chunk 302: 2,000,000 rows → 0 kept (0.0%) | Total read: 604,000,000\n",
      "Chunk 303: 2,000,000 rows → 0 kept (0.0%) | Total read: 606,000,000\n",
      "Chunk 304: 2,000,000 rows → 0 kept (0.0%) | Total read: 608,000,000\n",
      "Chunk 305: 2,000,000 rows → 0 kept (0.0%) | Total read: 610,000,000\n",
      "Chunk 306: 2,000,000 rows → 0 kept (0.0%) | Total read: 612,000,000\n",
      "Chunk 307: 2,000,000 rows → 0 kept (0.0%) | Total read: 614,000,000\n",
      "Chunk 308: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 616,000,000\n",
      "Chunk 309: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 618,000,000\n",
      "Chunk 310: 2,000,000 rows → 0 kept (0.0%) | Total read: 620,000,000\n",
      "Chunk 311: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 622,000,000\n",
      "Chunk 312: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 624,000,000\n",
      "Chunk 313: 2,000,000 rows → 0 kept (0.0%) | Total read: 626,000,000\n",
      "Chunk 314: 2,000,000 rows → 0 kept (0.0%) | Total read: 628,000,000\n",
      "Chunk 315: 2,000,000 rows → 0 kept (0.0%) | Total read: 630,000,000\n",
      "Chunk 316: 2,000,000 rows → 0 kept (0.0%) | Total read: 632,000,000\n",
      "Chunk 317: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 634,000,000\n",
      "Chunk 318: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 636,000,000\n",
      "Chunk 319: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 638,000,000\n",
      "Chunk 320: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 640,000,000\n",
      "Chunk 321: 2,000,000 rows → 0 kept (0.0%) | Total read: 642,000,000\n",
      "Chunk 322: 2,000,000 rows → 0 kept (0.0%) | Total read: 644,000,000\n",
      "Chunk 323: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 646,000,000\n",
      "Chunk 324: 2,000,000 rows → 0 kept (0.0%) | Total read: 648,000,000\n",
      "Chunk 325: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 650,000,000\n",
      "Chunk 326: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 652,000,000\n",
      "Chunk 327: 2,000,000 rows → 0 kept (0.0%) | Total read: 654,000,000\n",
      "Chunk 328: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 656,000,000\n",
      "Chunk 329: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 658,000,000\n",
      "Chunk 330: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 660,000,000\n",
      "Chunk 331: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 662,000,000\n",
      "Chunk 332: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 664,000,000\n",
      "Chunk 333: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 666,000,000\n",
      "Chunk 334: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 668,000,000\n",
      "Chunk 335: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 670,000,000\n",
      "Chunk 336: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 672,000,000\n",
      "Chunk 337: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 674,000,000\n",
      "Chunk 338: 2,000,000 rows → 2,000,000 kept (100.0%) | Total read: 676,000,000\n",
      "Chunk 339: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 678,000,000\n",
      "Chunk 340: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 680,000,000\n",
      "Chunk 341: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 682,000,000\n",
      "Chunk 342: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 684,000,000\n",
      "Chunk 343: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 686,000,000\n",
      "Chunk 344: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 688,000,000\n",
      "Chunk 345: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 690,000,000\n",
      "Chunk 346: 2,000,000 rows → 0 kept (0.0%) | Total read: 692,000,000\n",
      "Chunk 347: 2,000,000 rows → 0 kept (0.0%) | Total read: 694,000,000\n",
      "Chunk 348: 2,000,000 rows → 0 kept (0.0%) | Total read: 696,000,000\n",
      "Chunk 349: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 698,000,000\n",
      "Chunk 350: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 700,000,000\n",
      "Chunk 351: 2,000,000 rows → 0 kept (0.0%) | Total read: 702,000,000\n",
      "Chunk 352: 2,000,000 rows → 0 kept (0.0%) | Total read: 704,000,000\n",
      "Chunk 353: 2,000,000 rows → 0 kept (0.0%) | Total read: 706,000,000\n",
      "Chunk 354: 2,000,000 rows → 0 kept (0.0%) | Total read: 708,000,000\n",
      "Chunk 355: 2,000,000 rows → 0 kept (0.0%) | Total read: 710,000,000\n",
      "Chunk 356: 2,000,000 rows → 0 kept (0.0%) | Total read: 712,000,000\n",
      "Chunk 357: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 714,000,000\n",
      "Chunk 358: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 716,000,000\n",
      "Chunk 359: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 718,000,000\n",
      "Chunk 360: 2,000,000 rows → 0 kept (0.0%) | Total read: 720,000,000\n",
      "Chunk 361: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 722,000,000\n",
      "Chunk 362: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 724,000,000\n",
      "Chunk 363: 2,000,000 rows → 0 kept (0.0%) | Total read: 726,000,000\n",
      "Chunk 364: 2,000,000 rows → 0 kept (0.0%) | Total read: 728,000,000\n",
      "Chunk 365: 2,000,000 rows → 1,091,606 kept (54.6%) | Total read: 730,000,000\n",
      "Chunk 366: 2,000,000 rows → 1,908,394 kept (95.4%) | Total read: 732,000,000\n",
      "Chunk 367: 2,000,000 rows → 0 kept (0.0%) | Total read: 734,000,000\n",
      "Chunk 368: 2,000,000 rows → 0 kept (0.0%) | Total read: 736,000,000\n",
      "Chunk 369: 2,000,000 rows → 0 kept (0.0%) | Total read: 738,000,000\n",
      "Chunk 370: 2,000,000 rows → 0 kept (0.0%) | Total read: 740,000,000\n",
      "Chunk 371: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 742,000,000\n",
      "Chunk 372: 2,000,000 rows → 0 kept (0.0%) | Total read: 744,000,000\n",
      "Chunk 373: 2,000,000 rows → 91,606 kept (4.6%) | Total read: 746,000,000\n",
      "Chunk 374: 2,000,000 rows → 1,000,000 kept (50.0%) | Total read: 748,000,000\n",
      "Chunk 375: 2,000,000 rows → 908,394 kept (45.4%) | Total read: 750,000,000\n",
      "Chunk 376: 2,000,000 rows → 0 kept (0.0%) | Total read: 752,000,000\n",
      "Chunk 377: 2,000,000 rows → 0 kept (0.0%) | Total read: 754,000,000\n",
      "Chunk 378: 2,000,000 rows → 0 kept (0.0%) | Total read: 756,000,000\n",
      "Chunk 379: 2,000,000 rows → 0 kept (0.0%) | Total read: 758,000,000\n",
      "Chunk 380: 1,908,394 rows → 0 kept (0.0%) | Total read: 759,908,394\n",
      "\n",
      "Concatenating chunks...\n",
      "✓ Complete! Processed 218,477,461 rows in 2043.3s (34.1 min)\n",
      "  Columns: 14 | Year range: 2006-2015 | States: 14\n",
      "\n",
      "Saving to parquet: ../data/raw\\arcos_filtered_2006_2015.parquet\n",
      "✓ Saved successfully!\n",
      "  File size: 2144.3 MB\n",
      "\n",
      "============================================================\n",
      "✓ COMPLETE in 2312.0 seconds (38.5 minutes)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Extracting ARCOS data (Optimized)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Check if filtering is enabled\n",
    "use_filtering = 'states_filter' in locals() and states_filter is not None\n",
    "if use_filtering:\n",
    "    print(f\"FILTERING ENABLED: {len(states_filter)} states, years {year_min}-{year_max}\")\n",
    "else:\n",
    "    print(\"NO FILTERING: Extracting all data\")\n",
    "print()\n",
    "\n",
    "# Optimized dtypes - no category to avoid memory overhead\n",
    "dtypes = {\n",
    "    'BUYER_STATE': str,\n",
    "    'BUYER_COUNTY': str,\n",
    "    'DRUG_NAME': str,\n",
    "    'DRUG_CODE': str,\n",
    "    'TRANSACTION_CODE': str,\n",
    "    'Measure': str,\n",
    "    'NDC_NO': str,\n",
    "    'MME': 'float32',\n",
    "    'DOSAGE_UNIT': 'float32',\n",
    "    'MME_Conversion_Factor': 'float32',\n",
    "    'Dosage_Strength': 'float32',\n",
    "    'CALC_BASE_WT_IN_GM': 'float32'\n",
    "}\n",
    "\n",
    "# Read with chunking for progress tracking\n",
    "chunk_size = 2_000_000  # 2M rows per chunk\n",
    "chunks = []\n",
    "total_rows = 0\n",
    "chunk_num = 0\n",
    "\n",
    "print(\"Reading TSV file in chunks (2M rows each)...\")\n",
    "print()\n",
    "\n",
    "reader = pd.read_csv(\n",
    "    tsv_path,\n",
    "    sep='\\t',\n",
    "    usecols=columns_to_keep,\n",
    "    dtype=dtypes,\n",
    "    engine='c',\n",
    "    chunksize=chunk_size,\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "for chunk in reader:\n",
    "    chunk_num += 1\n",
    "    chunk_rows = len(chunk)\n",
    "    total_rows += chunk_rows\n",
    "    \n",
    "    # Extract year from dates\n",
    "    chunk['year'] = pd.to_datetime(chunk['TRANSACTION_DATE'], format='%Y-%m-%d', errors='coerce').dt.year.astype('Int16')\n",
    "    \n",
    "    # Apply filtering if enabled\n",
    "    if use_filtering:\n",
    "        chunk = chunk[\n",
    "            (chunk['BUYER_STATE'].isin(states_filter)) & \n",
    "            (chunk['year'] >= year_min) & \n",
    "            (chunk['year'] <= year_max)\n",
    "        ]\n",
    "    \n",
    "    chunks.append(chunk)\n",
    "    \n",
    "    # Print progress\n",
    "    if use_filtering:\n",
    "        kept_rows = len(chunk)\n",
    "        print(f\"Chunk {chunk_num}: {chunk_rows:,} rows → {kept_rows:,} kept ({kept_rows/chunk_rows*100:.1f}%) | Total read: {total_rows:,}\")\n",
    "    else:\n",
    "        print(f\"Chunk {chunk_num}: {chunk_rows:,} rows | Total read: {total_rows:,} | Elapsed: {time.time()-start_time:.1f}s\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Concatenate all chunks\n",
    "print(\"Concatenating chunks...\")\n",
    "df_all = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "process_time = time.time() - start_time\n",
    "print(f\"✓ Complete! Processed {len(df_all):,} rows in {process_time:.1f}s ({process_time/60:.1f} min)\")\n",
    "print(f\"  Columns: {df_all.shape[1]} | Year range: {df_all['year'].min()}-{df_all['year'].max()} | States: {df_all['BUYER_STATE'].nunique()}\")\n",
    "\n",
    "# Save to parquet\n",
    "output_dir = '../data/raw'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "if use_filtering:\n",
    "    output_file = os.path.join(output_dir, f'arcos_filtered_{year_min}_{year_max}.parquet')\n",
    "else:\n",
    "    output_file = os.path.join(output_dir, 'arcos_all_extracted.parquet')\n",
    "\n",
    "print(f\"\\nSaving to parquet: {output_file}\")\n",
    "df_all.to_parquet(output_file, index=False, compression='snappy')\n",
    "\n",
    "file_size = os.path.getsize(output_file)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"✓ Saved successfully!\")\n",
    "print(f\"  File size: {file_size / (1024**2):.1f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"✓ COMPLETE in {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20421e73",
   "metadata": {},
   "source": [
    "## Step 6: Verify Saved File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae399f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying saved file...\n",
      "============================================================\n",
      "\n",
      "File loaded successfully!\n",
      "  Shape: 218,477,461 rows × 14 columns\n",
      "  Year range: 2006 - 2015\n",
      "  States: 14 unique states\n",
      "  Counties: 777 unique counties\n",
      "  Drugs: ['BUPRENORPHINE', 'CODEINE', 'DIHYDROCODEINE', 'FENTANYL', 'HYDROCODONE', 'HYDROMORPHONE', 'LEVORPHANOL', 'MEPERIDINE', 'METHADONE', 'MORPHINE', 'OPIUM, POWDERED', 'OXYCODONE', 'OXYMORPHONE', 'TAPENTADOL']\n",
      "\n",
      "First 5 rows:\n",
      "  BUYER_STATE BUYER_COUNTY      DRUG_NAME  Dosage_Strength  DOSAGE_UNIT  \\\n",
      "0          CA      ALAMEDA      METHADONE              0.0          0.0   \n",
      "1          CA      ALAMEDA     MEPERIDINE              0.0          0.0   \n",
      "2          CA      ALAMEDA       FENTANYL              0.0          0.0   \n",
      "3          CA      ALAMEDA  HYDROMORPHONE              0.0          0.0   \n",
      "4          CA      ALAMEDA  HYDROMORPHONE              0.0          0.0   \n",
      "\n",
      "           MME  year  \n",
      "0   458.827209  2015  \n",
      "1    39.217499  2015  \n",
      "2    10.000000  2015  \n",
      "3  3546.399902  2015  \n",
      "4    21.278400  2015  \n",
      "\n",
      "Summary statistics:\n",
      "  Total MME: 4,415,495,352,614,912\n",
      "  Total dosage units: 611,181,723,648\n",
      "\n",
      "✓ File is valid and ready for analysis!\n"
     ]
    }
   ],
   "source": [
    "# Verify the saved file\n",
    "print(\"Verifying saved file...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_verify = pd.read_parquet(output_file)\n",
    "\n",
    "print(f\"\\nFile loaded successfully!\")\n",
    "print(f\"  Shape: {df_verify.shape[0]:,} rows × {df_verify.shape[1]} columns\")\n",
    "print(f\"  Year range: {df_verify['year'].min()} - {df_verify['year'].max()}\")\n",
    "print(f\"  States: {df_verify['BUYER_STATE'].nunique()} unique states\")\n",
    "print(f\"  Counties: {df_verify['BUYER_COUNTY'].nunique()} unique counties\")\n",
    "print(f\"  Drugs: {sorted(df_verify['DRUG_NAME'].unique())}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_verify[['BUYER_STATE', 'BUYER_COUNTY', 'DRUG_NAME', 'Dosage_Strength', 'DOSAGE_UNIT', 'MME', 'year']].head())\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(f\"  Total MME: {df_verify['MME'].sum():,.0f}\")\n",
    "print(f\"  Total dosage units: {df_verify['DOSAGE_UNIT'].sum():,.0f}\")\n",
    "\n",
    "print(\"\\n✓ File is valid and ready for analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
